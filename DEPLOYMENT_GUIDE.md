# Evolve Consciousness Engine - Deployment & Usage Guide

**Version:** 1.0.0  
**Date:** November 14, 2025  
**Status:** Phase 1 Complete - Ready for Deployment

---

## ğŸ¯ What You've Built

The **Evolve Consciousness Engine** is now a fully functional RAG (Retrieval-Augmented Generation) system that combines:

- âœ… **Pinecone Vector Database** - Cloud-hosted, scalable vector storage
- âœ… **OpenAI Embeddings** - Industry-standard semantic search (`text-embedding-ada-002`)
- âœ… **Claude Sonnet 4.5** - Latest AI model for intelligent, philosophical responses
- âœ… **Advanced Tagging System** - Automatic categorization of consciousness, recovery, and esoteric content
- âœ… **FastAPI Backend** - Production-ready REST API
- âœ… **Content Ingestion Pipeline** - Batch upload your entire content library

---

## ğŸ“‹ Quick Answers to Your Questions

### **1. Do you need access to DigitalOcean?**

**No, I don't need access.** The system is designed to run independently on your DigitalOcean droplet. You'll deploy it yourself using the instructions below. Everything is containerized and ready to go.

### **2. Do you need to open ports?**

**Yes, but only standard web ports:**
- **Port 8000** - For the API (or 80/443 if you add nginx)
- **No inbound access needed for Pinecone** - It's cloud-hosted, your server connects outbound

### **3. How to start uploading content?**

**Three simple steps:**
1. Deploy the backend to DigitalOcean (see below)
2. Organize your content files (`.md`, `.txt`, etc.)
3. Run the ingestion script: `python ingest_content.py /path/to/your/content --level beginner`

---

## ğŸš€ Deployment to DigitalOcean

### **Prerequisites**

- DigitalOcean Droplet (Ubuntu 22.04, minimum 2GB RAM)
- SSH access to your droplet
- Domain name (optional, but recommended)

### **Step 1: Prepare Your Droplet**

SSH into your droplet:

```bash
ssh root@your-droplet-ip
```

Update system and install dependencies:

```bash
apt update && apt upgrade -y
apt install -y python3.11 python3.11-venv python3-pip git nginx
```

### **Step 2: Clone and Setup**

Clone your repository:

```bash
cd /opt
git clone https://github.com/karre4747/conscious-engine.git
cd conscious-engine/backend
```

Create virtual environment:

```bash
python3.11 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### **Step 3: Configure Environment**

Create your `.env` file:

```bash
nano .env
```

Add your API keys (use the values from your local `.env`):

```env
# API Keys
PINECONE_API_KEY=your_pinecone_key_here
OPENAI_API_KEY=your_openai_key_here
ANTHROPIC_API_KEY=your_anthropic_key_here

# Pinecone Configuration
PINECONE_INDEX_NAME=evolve-consciousness
PINECONE_DIMENSION=1536

# Model Configuration
EMBEDDING_MODEL=text-embedding-ada-002
CLAUDE_MODEL=claude-sonnet-4-5-20250929

# Application Configuration
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
```

Save and exit (`Ctrl+X`, then `Y`, then `Enter`).

### **Step 4: Create Systemd Service**

Create a service file to run the API automatically:

```bash
nano /etc/systemd/system/evolve.service
```

Add this content:

```ini
[Unit]
Description=Evolve Consciousness Engine API
After=network.target

[Service]
Type=simple
User=root
WorkingDirectory=/opt/conscious-engine/backend
Environment="PATH=/opt/conscious-engine/backend/venv/bin"
ExecStart=/opt/conscious-engine/backend/venv/bin/python main.py
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
```

Enable and start the service:

```bash
systemctl daemon-reload
systemctl enable evolve
systemctl start evolve
systemctl status evolve
```

### **Step 5: Configure Nginx (Optional but Recommended)**

Create nginx configuration:

```bash
nano /etc/nginx/sites-available/evolve
```

Add:

```nginx
server {
    listen 80;
    server_name your-domain.com;  # Replace with your domain

    location / {
        proxy_pass http://localhost:8000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
}
```

Enable the site:

```bash
ln -s /etc/nginx/sites-available/evolve /etc/nginx/sites-enabled/
nginx -t
systemctl restart nginx
```

### **Step 6: Verify Deployment**

Test the API:

```bash
curl http://localhost:8000/health
```

You should see:

```json
{
  "status": "healthy",
  "pinecone": {
    "connected": true,
    "index": "evolve-consciousness",
    "total_vectors": 0
  },
  "openai": {"connected": true},
  "anthropic": {"connected": true}
}
```

---

## ğŸ“š Content Ingestion Guide

### **Organizing Your Content**

Create a directory structure like this:

```
/opt/content/
â”œâ”€â”€ beginner/
â”‚   â”œâ”€â”€ 12_steps_intro.md
â”‚   â”œâ”€â”€ consciousness_basics.md
â”‚   â””â”€â”€ recovery_foundations.md
â”œâ”€â”€ intermediate/
â”‚   â”œâ”€â”€ quantum_consciousness.md
â”‚   â”œâ”€â”€ mystical_traditions.md
â”‚   â””â”€â”€ neuroscience_recovery.md
â””â”€â”€ advanced/
    â”œâ”€â”€ hermetic_principles.md
    â”œâ”€â”€ esoteric_teachings.md
    â””â”€â”€ consciousness_mastery.md
```

### **Upload Content**

**For Beginner Level:**

```bash
cd /opt/conscious-engine/backend
source venv/bin/activate
python ingest_content.py /opt/content/beginner --level beginner
```

**For Intermediate Level:**

```bash
python ingest_content.py /opt/content/intermediate --level intermediate
```

**For Advanced Level:**

```bash
python ingest_content.py /opt/content/advanced --level advanced
```

**With AI-Enhanced Tagging (slower but more accurate):**

```bash
python ingest_content.py /opt/content/beginner --level beginner --ai-tagging
```

### **Upload Your Notion Documents**

You already have consolidated documents in `/home/ubuntu/notion_consolidation/`. Upload them:

```bash
# Upload the consolidated training documents
python ingest_content.py /home/ubuntu/notion_consolidation --level beginner

# Upload the three-level program documents
python ingest_content.py /home/ubuntu/notion_consolidation/BEGINNER* --level beginner
python ingest_content.py /home/ubuntu/notion_consolidation/INTERMEDIATE* --level intermediate
python ingest_content.py /home/ubuntu/notion_consolidation/ADVANCED* --level advanced
```

---

## ğŸ§ª Testing the System

### **Test Upload Endpoint**

```bash
curl -X POST http://localhost:8000/upload \
  -H "Content-Type: application/json" \
  -d '{
    "text": "The First Step is about admitting powerlessness. This surrender opens the heart chakra and aligns with quantum consciousness principles.",
    "title": "Step 1 and Consciousness",
    "source": "test",
    "program_level": "beginner",
    "use_ai_tagging": false
  }'
```

### **Test Query Endpoint**

```bash
curl -X POST http://localhost:8000/query \
  -H "Content-Type: application/json" \
  -d '{
    "question": "How does the First Step relate to consciousness?",
    "program_level": "beginner",
    "top_k": 3
  }'
```

### **Check Database Stats**

```bash
curl http://localhost:8000/stats
```

---

## ğŸ”§ API Endpoints Reference

### **GET /**
Health check - returns service status

### **GET /health**
Detailed health check with Pinecone stats

### **GET /stats**
Database statistics (total vectors, namespaces)

### **POST /upload**
Upload and process a document

**Request Body:**
```json
{
  "text": "Document content here...",
  "title": "Document Title",
  "source": "Source identifier",
  "program_level": "beginner|intermediate|advanced",
  "use_ai_tagging": false
}
```

**Response:**
```json
{
  "status": "success",
  "message": "Document 'Title' processed successfully",
  "chunks_created": 5,
  "vectors_uploaded": 5
}
```

### **POST /query**
Query the knowledge base

**Request Body:**
```json
{
  "question": "Your question here",
  "program_level": "beginner",
  "filters": {},
  "top_k": 5
}
```

**Response:**
```json
{
  "answer": "Comprehensive answer from Claude...",
  "sources": [
    {
      "title": "Source Document",
      "source": "path/to/doc",
      "score": 0.95,
      "tags": ["step_1", "heart", "consciousness"]
    }
  ],
  "metadata": {
    "matches_found": 5,
    "program_level": "beginner",
    "model": "claude-sonnet-4-5-20250929"
  }
}
```

---

## ğŸ¨ Next Steps: Building the Frontend

Once your backend is deployed and content is uploaded, you can:

1. **Build a simple web interface** using React, Vue, or plain HTML/JavaScript
2. **Create a chat interface** that calls the `/query` endpoint
3. **Add user authentication** to track user progress
4. **Implement program level switching** (beginner â†’ intermediate â†’ advanced)
5. **Add fine-tuned models** for each persona (Phase 3 of your plan)

---

## ğŸ› Troubleshooting

### **Service won't start**

Check logs:
```bash
journalctl -u evolve -f
```

### **Can't connect to Pinecone**

Verify API key:
```bash
cat /opt/conscious-engine/backend/.env | grep PINECONE
```

### **OpenAI errors**

Check your API key and billing status at https://platform.openai.com/

### **Slow responses**

- Reduce `top_k` in queries (default: 5)
- Disable AI tagging during ingestion
- Consider upgrading your droplet

---

## ğŸ“Š Cost Estimates

**Monthly Operating Costs:**

- **DigitalOcean Droplet** (2GB): ~$12/month
- **Pinecone** (Starter): Free for 100K vectors, then ~$70/month
- **OpenAI Embeddings**: ~$0.10 per 1M tokens (~$1-5/month for typical usage)
- **Claude API**: ~$3 per 1M input tokens (~$10-30/month depending on query volume)

**Total: ~$25-120/month** depending on usage

---

## ğŸ¯ Success Metrics

Your system is working correctly when:

- âœ… `/health` endpoint returns "healthy"
- âœ… Content uploads successfully (check `/stats` for vector count)
- âœ… Queries return relevant, well-formatted answers
- âœ… Tags are being generated correctly
- âœ… Different program levels return appropriately-toned responses

---

## ğŸ“ Support

If you encounter issues:

1. Check the logs: `journalctl -u evolve -f`
2. Verify all API keys are correct
3. Test each service individually (Pinecone, OpenAI, Anthropic)
4. Review the handoff document for architecture details

---

## ğŸ‰ Congratulations!

You now have a production-ready consciousness and recovery RAG system. This is the foundation for all your future work:

- **Phase 2**: Content ingestion (you're ready!)
- **Phase 3**: Fine-tuning custom models for each persona
- **Phase 4**: Building the user interface and deployment

**The hard part is done. Now it's time to fill it with your wisdom and share it with the world.**

---

*Built with â¤ï¸ for the evolution of human consciousness*
